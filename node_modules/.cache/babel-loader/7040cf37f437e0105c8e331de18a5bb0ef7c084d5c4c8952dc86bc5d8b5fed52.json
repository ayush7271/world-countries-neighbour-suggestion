{"ast":null,"code":"var _jsxFileName = \"/home/ayush/Desktop/cl/world-countries-neighbour-suggestion/src/EyeDetection.jsx\",\n  _s = $RefreshSig$();\n// import React, { useRef, useEffect, useState } from 'react';\n// import Webcam from 'react-webcam';\n// import * as faceapi from 'face-api.js';\n\n// const EyeDetection = ({ setEyeBlinkCount }) => {\n// \tconst webcamRef = useRef(null);\n// \tconst [areEyesOpen, setAreEyesOpen] = useState(false);\n// \tconst [eyeHeight, setEyeHeight] = useState();\n// \tconst [eyeValue, setEyeValue] = useState();\n// \tconst [imgValue, setImgValue] = useState('');\n// \tuseEffect(() => {\n// \t\tconst loadModels = async () => {\n// \t\t\tawait faceapi.nets.tinyFaceDetector.loadFromUri('/models');\n// \t\t\tawait faceapi.nets.faceLandmark68Net.loadFromUri('/models');\n// \t\t\tawait faceapi.nets.faceExpressionNet.loadFromUri('/models');\n// \t\t};\n\n// \t\tconst startDetection = async () => {\n// \t\t\tif (webcamRef?.current && webcamRef?.current?.video?.readyState === 4) {\n// \t\t\t\tconst videoEl = webcamRef.current.video;\n// \t\t\t\tconst displaySize = { width: videoEl.width, height: videoEl.height };\n// \t\t\t\tfaceapi.matchDimensions(webcamRef.current.video, displaySize);\n\n// \t\t\t\ttry {\n// \t\t\t\t\tconst detections = await faceapi\n// \t\t\t\t\t\t.detectAllFaces(\n// \t\t\t\t\t\t\t// .detectSingleFace\n// \t\t\t\t\t\t\tvideoEl,\n// \t\t\t\t\t\t\tnew faceapi.TinyFaceDetectorOptions()\n// \t\t\t\t\t\t)\n// \t\t\t\t\t\t.withFaceLandmarks()\n// \t\t\t\t\t\t// .withFaceDescriptors()\n// \t\t\t\t\t\t.withFaceExpressions();\n\n// \t\t\t\t\tif (detections.length > 0) {\n// \t\t\t\t\t\tconst landmarks = detections[0].landmarks;\n// \t\t\t\t\t\tconst leftEye = landmarks.getLeftEye();\n// \t\t\t\t\t\tconst rightEye = landmarks.getRightEye();\n// \t\t\t\t\t\tconst leftEyeOpen = isEyeOpen(leftEye);\n// \t\t\t\t\t\tconst rightEyeOpen = isEyeOpen(rightEye);\n// \t\t\t\t\t\tsetAreEyesOpen(leftEyeOpen && rightEyeOpen);\n// \t\t\t\t\t\tconst expressions = detections[0].expressions;\n// \t\t\t\t\t\t// Here you can use expressions to get the detected emotions\n// \t\t\t\t\t\t// console.log('Detected expressions:', expressions);\n// \t\t\t\t\t} else {\t\n// \t\t\t\t\t\tsetAreEyesOpen(false);\n// \t\t\t\t\t\tconsole.log('No user detected');\n// \t\t\t\t\t}\n// \t\t\t\t} catch (error) {\n// \t\t\t\t\tconsole.error('Error detecting faces:', error);\n// \t\t\t\t}\n// \t\t\t}\n// \t\t\trequestAnimationFrame(startDetection);\n// \t\t};\n\n// \t\tloadModels();\n// \t\tstartDetection();\n// \t}, []);\n\n// \tconst isEyeOpen = (eyeLandmarks) => {\n// \t\tconst top = eyeLandmarks[1].y;\n// \t\tconst bottom = eyeLandmarks[5].y;\n// \t\tconst height = bottom - top;\n// \t\tconst threshold = height * 0.4; // You can adjust this threshold based on your needs\n// \t\tsetEyeValue(Math.floor(height));\n// \t\t// console.log(Math.round(threshold),\"threshold\")\n// \t\t// if (Math.floor(height) === 6) {\n// \t\t// \tsetEyeHeight(true);\n// \t\t// } else {\n// \t\t// \tsetTimeout(()=>{\n// \t\t// \t\tsetEyeHeight(false);\n// \t\t// \t},5000)\n// \t\t// }\n\n// \t\t// console.log(height ,eyeLandmarks[4].y - eyeLandmarks[1].y)\n\n// \t\t// console.log(top,bottom,height,threshold)\n// \t\t// return eyeLandmarks[4].y - eyeLandmarks[1].y > threshold;\n// \t};\n// \t//\n// \tconsole.log(eyeValue, 'eyevalue');\n// \tuseEffect(() => {\n// \t\tif (eyeValue === 6) {\n// \t\t\tsetEyeHeight(true);\n// \t\t} else {\n// \t\t\tsetEyeHeight(false);\n// \t\t}\n// \t}, [eyeValue]);\n// \tconsole.log(eyeValue)\n// \treturn (\n// \t\t<div className={`${eyeValue===5?'black':'white'}`}>\n// \t\t\t\t<Webcam\n// \t\t\t\t\tref={webcamRef}\n// \t\t\t\t\tmirrored={true}\n// \t\t\t\t\tclassName={`w-[95%] h-full m-auto `}\n// \t\t\t\t\tstyle={{ transform: 'scaleX(1)' }}\n// \t\t\t\t/>\n\n// \t\t\t{/* <Webcam ref={webcamRef} mirrored={true} style={{ display: 'block', width:'200px', height:'200px', margin: 'auto', maxWidth: '100%', borderRadius:'50%' }} /> */}\n// \t\t\t{/* <Webcam ref={webcamRef} mirrored={true} style={{width:'200px'}} /> */}\n// \t\t\t{/* {eyeHeight ? (\n// \t\t\t\t<p className='text-[#ffffff]'>Eyes are open.</p>\n// \t\t\t) : (\n// \t\t\t\t<p className='text-[#ffffff]'>fit your self in frame.</p>\n// \t\t\t)} */}\n// \t\t\t{/* <p className='text-[#373737]'>\n// \t\t\t\they {eyeValue}\n// \t\t\t\t{eyeHeight}\n// \t\t\t</p> */}\n\n// \t\t</div>\n// \t);\n// };\n\n// export default EyeDetection;\n\nimport React, { useRef, useEffect, useState } from 'react';\nimport Webcam from 'react-webcam';\nimport * as faceapi from 'face-api.js';\nimport ProgressBar from './ProgressBar';\nimport { jsxDEV as _jsxDEV } from \"react/jsx-dev-runtime\";\nconst EyeDetection = ({\n  setEyeBlinkCount\n}) => {\n  _s();\n  const webcamRef = useRef(null);\n  const [areEyesOpen, setAreEyesOpen] = useState(false);\n  const [eyeHeight, setEyeHeight] = useState();\n  const [eyeValue, setEyeValue] = useState();\n  const [imgValue, setImgValue] = useState();\n  useEffect(() => {\n    const loadModels = async () => {\n      await faceapi.nets.tinyFaceDetector.loadFromUri('/models');\n      await faceapi.nets.faceLandmark68Net.loadFromUri('/models');\n      await faceapi.nets.faceExpressionNet.loadFromUri('/models');\n    };\n    const startDetection = async () => {\n      if (webcamRef.current && webcamRef.current.video.readyState === 4) {\n        const videoEl = webcamRef.current.video;\n        const displaySize = {\n          width: videoEl.width,\n          height: videoEl.height\n        };\n        faceapi.matchDimensions(videoEl, displaySize);\n        try {\n          const detections = await faceapi.detectAllFaces(videoEl, new faceapi.TinyFaceDetectorOptions()).withFaceLandmarks().withFaceExpressions();\n          if (detections.length > 0) {\n            const landmarks = detections[0].landmarks;\n            const leftEye = landmarks.getLeftEye();\n            const rightEye = landmarks.getRightEye();\n            const leftEyeOpen = isEyeOpen(leftEye);\n            const rightEyeOpen = isEyeOpen(rightEye);\n            setAreEyesOpen(leftEyeOpen && rightEyeOpen);\n          } else {\n            setAreEyesOpen(false);\n            console.log('No user detected');\n          }\n        } catch (error) {\n          console.error('Error detecting faces:', error);\n        }\n      }\n    };\n    const interval = setInterval(async () => {\n      await startDetection();\n    }, 100);\n    loadModels();\n    startDetection();\n\n    // Clean up function to stop interval when component unmounts\n    return () => {\n      clearInterval(interval);\n    };\n  }, []);\n  const isEyeOpen = eyeLandmarks => {\n    const top = eyeLandmarks[1].y;\n    const bottom = eyeLandmarks[5].y;\n    const height = bottom - top;\n    const threshold = height * 0.4; // You can adjust this threshold based on your needs\n    setEyeValue(Math.floor(height));\n    console.log(Math.floor(height), \"Math.floor(height)\");\n    return eyeLandmarks[4].y - eyeLandmarks[1].y > threshold;\n  };\n  useEffect(() => {\n    if (eyeValue === 5) {\n      setEyeHeight(true);\n    } else {\n      setEyeHeight(false);\n    }\n  }, [eyeValue]);\n  return /*#__PURE__*/_jsxDEV(\"div\", {\n    style: {\n      height: '765px',\n      background: eyeValue === 5 ? 'black' : 'green'\n    },\n    children: /*#__PURE__*/_jsxDEV(Webcam, {\n      ref: webcamRef,\n      mirrored: true,\n      className: `w-[95%] h-full m-auto `,\n      style: {\n        transform: 'scaleX(1)'\n      }\n    }, void 0, false, {\n      fileName: _jsxFileName,\n      lineNumber: 199,\n      columnNumber: 2\n    }, this)\n  }, void 0, false, {\n    fileName: _jsxFileName,\n    lineNumber: 198,\n    columnNumber: 2\n  }, this);\n};\n_s(EyeDetection, \"x/Jc0KASP6MVvU0XAF5OL1k6tCs=\");\n_c = EyeDetection;\nexport default EyeDetection;\nvar _c;\n$RefreshReg$(_c, \"EyeDetection\");","map":{"version":3,"names":["React","useRef","useEffect","useState","Webcam","faceapi","ProgressBar","jsxDEV","_jsxDEV","EyeDetection","setEyeBlinkCount","_s","webcamRef","areEyesOpen","setAreEyesOpen","eyeHeight","setEyeHeight","eyeValue","setEyeValue","imgValue","setImgValue","loadModels","nets","tinyFaceDetector","loadFromUri","faceLandmark68Net","faceExpressionNet","startDetection","current","video","readyState","videoEl","displaySize","width","height","matchDimensions","detections","detectAllFaces","TinyFaceDetectorOptions","withFaceLandmarks","withFaceExpressions","length","landmarks","leftEye","getLeftEye","rightEye","getRightEye","leftEyeOpen","isEyeOpen","rightEyeOpen","console","log","error","interval","setInterval","clearInterval","eyeLandmarks","top","y","bottom","threshold","Math","floor","style","background","children","ref","mirrored","className","transform","fileName","_jsxFileName","lineNumber","columnNumber","_c","$RefreshReg$"],"sources":["/home/ayush/Desktop/cl/world-countries-neighbour-suggestion/src/EyeDetection.jsx"],"sourcesContent":["// import React, { useRef, useEffect, useState } from 'react';\n// import Webcam from 'react-webcam';\n// import * as faceapi from 'face-api.js';\n\n// const EyeDetection = ({ setEyeBlinkCount }) => {\n// \tconst webcamRef = useRef(null);\n// \tconst [areEyesOpen, setAreEyesOpen] = useState(false);\n// \tconst [eyeHeight, setEyeHeight] = useState();\n// \tconst [eyeValue, setEyeValue] = useState();\n// \tconst [imgValue, setImgValue] = useState('');\n// \tuseEffect(() => {\n// \t\tconst loadModels = async () => {\n// \t\t\tawait faceapi.nets.tinyFaceDetector.loadFromUri('/models');\n// \t\t\tawait faceapi.nets.faceLandmark68Net.loadFromUri('/models');\n// \t\t\tawait faceapi.nets.faceExpressionNet.loadFromUri('/models');\n// \t\t};\n\n// \t\tconst startDetection = async () => {\n// \t\t\tif (webcamRef?.current && webcamRef?.current?.video?.readyState === 4) {\n// \t\t\t\tconst videoEl = webcamRef.current.video;\n// \t\t\t\tconst displaySize = { width: videoEl.width, height: videoEl.height };\n// \t\t\t\tfaceapi.matchDimensions(webcamRef.current.video, displaySize);\n\n// \t\t\t\ttry {\n// \t\t\t\t\tconst detections = await faceapi\n// \t\t\t\t\t\t.detectAllFaces(\n// \t\t\t\t\t\t\t// .detectSingleFace\n// \t\t\t\t\t\t\tvideoEl,\n// \t\t\t\t\t\t\tnew faceapi.TinyFaceDetectorOptions()\n// \t\t\t\t\t\t)\n// \t\t\t\t\t\t.withFaceLandmarks()\n// \t\t\t\t\t\t// .withFaceDescriptors()\n// \t\t\t\t\t\t.withFaceExpressions();\n\n// \t\t\t\t\tif (detections.length > 0) {\n// \t\t\t\t\t\tconst landmarks = detections[0].landmarks;\n// \t\t\t\t\t\tconst leftEye = landmarks.getLeftEye();\n// \t\t\t\t\t\tconst rightEye = landmarks.getRightEye();\n// \t\t\t\t\t\tconst leftEyeOpen = isEyeOpen(leftEye);\n// \t\t\t\t\t\tconst rightEyeOpen = isEyeOpen(rightEye);\n// \t\t\t\t\t\tsetAreEyesOpen(leftEyeOpen && rightEyeOpen);\n// \t\t\t\t\t\tconst expressions = detections[0].expressions;\n// \t\t\t\t\t\t// Here you can use expressions to get the detected emotions\n// \t\t\t\t\t\t// console.log('Detected expressions:', expressions);\n// \t\t\t\t\t} else {\t\n// \t\t\t\t\t\tsetAreEyesOpen(false);\n// \t\t\t\t\t\tconsole.log('No user detected');\n// \t\t\t\t\t}\n// \t\t\t\t} catch (error) {\n// \t\t\t\t\tconsole.error('Error detecting faces:', error);\n// \t\t\t\t}\n// \t\t\t}\n// \t\t\trequestAnimationFrame(startDetection);\n// \t\t};\n\n// \t\tloadModels();\n// \t\tstartDetection();\n// \t}, []);\n\n// \tconst isEyeOpen = (eyeLandmarks) => {\n// \t\tconst top = eyeLandmarks[1].y;\n// \t\tconst bottom = eyeLandmarks[5].y;\n// \t\tconst height = bottom - top;\n// \t\tconst threshold = height * 0.4; // You can adjust this threshold based on your needs\n// \t\tsetEyeValue(Math.floor(height));\n// \t\t// console.log(Math.round(threshold),\"threshold\")\n// \t\t// if (Math.floor(height) === 6) {\n// \t\t// \tsetEyeHeight(true);\n// \t\t// } else {\n// \t\t// \tsetTimeout(()=>{\n// \t\t// \t\tsetEyeHeight(false);\n// \t\t// \t},5000)\n// \t\t// }\n\n// \t\t// console.log(height ,eyeLandmarks[4].y - eyeLandmarks[1].y)\n\n// \t\t// console.log(top,bottom,height,threshold)\n// \t\t// return eyeLandmarks[4].y - eyeLandmarks[1].y > threshold;\n// \t};\n// \t//\n// \tconsole.log(eyeValue, 'eyevalue');\n// \tuseEffect(() => {\n// \t\tif (eyeValue === 6) {\n// \t\t\tsetEyeHeight(true);\n// \t\t} else {\n// \t\t\tsetEyeHeight(false);\n// \t\t}\n// \t}, [eyeValue]);\n// \tconsole.log(eyeValue)\n// \treturn (\n// \t\t<div className={`${eyeValue===5?'black':'white'}`}>\n// \t\t\t\t<Webcam\n// \t\t\t\t\tref={webcamRef}\n// \t\t\t\t\tmirrored={true}\n// \t\t\t\t\tclassName={`w-[95%] h-full m-auto `}\n// \t\t\t\t\tstyle={{ transform: 'scaleX(1)' }}\n// \t\t\t\t/>\n\t\t\t\n// \t\t\t{/* <Webcam ref={webcamRef} mirrored={true} style={{ display: 'block', width:'200px', height:'200px', margin: 'auto', maxWidth: '100%', borderRadius:'50%' }} /> */}\n// \t\t\t{/* <Webcam ref={webcamRef} mirrored={true} style={{width:'200px'}} /> */}\n// \t\t\t{/* {eyeHeight ? (\n// \t\t\t\t<p className='text-[#ffffff]'>Eyes are open.</p>\n// \t\t\t) : (\n// \t\t\t\t<p className='text-[#ffffff]'>fit your self in frame.</p>\n// \t\t\t)} */}\n// \t\t\t{/* <p className='text-[#373737]'>\n// \t\t\t\they {eyeValue}\n// \t\t\t\t{eyeHeight}\n// \t\t\t</p> */}\n\t\t\t\t\n// \t\t</div>\n// \t);\n// };\n\n// export default EyeDetection;\n\n\nimport React, { useRef, useEffect, useState } from 'react';\nimport Webcam from 'react-webcam';\nimport * as faceapi from 'face-api.js';\nimport ProgressBar from './ProgressBar';\n\nconst EyeDetection = ({ setEyeBlinkCount }) => {\n  const webcamRef = useRef(null);\n  const [areEyesOpen, setAreEyesOpen] = useState(false);\n  const [eyeHeight, setEyeHeight] = useState();\n  const [eyeValue, setEyeValue] = useState();\n  const [imgValue, setImgValue] = useState();\n\n  useEffect(() => {\n    const loadModels = async () => {\n      await faceapi.nets.tinyFaceDetector.loadFromUri('/models');\n      await faceapi.nets.faceLandmark68Net.loadFromUri('/models');\n      await faceapi.nets.faceExpressionNet.loadFromUri('/models');\n    };\n\n    const startDetection = async () => {\n      if (webcamRef.current && webcamRef.current.video.readyState === 4) {\n        const videoEl = webcamRef.current.video;\n        const displaySize = { width: videoEl.width, height: videoEl.height };\n        faceapi.matchDimensions(videoEl, displaySize);\n\n        try {\n          const detections = await faceapi\n            .detectAllFaces(videoEl, new faceapi.TinyFaceDetectorOptions())\n            .withFaceLandmarks()\n            .withFaceExpressions();\n\n          if (detections.length > 0) {\n            const landmarks = detections[0].landmarks;\n            const leftEye = landmarks.getLeftEye();\n            const rightEye = landmarks.getRightEye();\n            const leftEyeOpen = isEyeOpen(leftEye);\n            const rightEyeOpen = isEyeOpen(rightEye);\n            setAreEyesOpen(leftEyeOpen && rightEyeOpen);\n          } else {\n            setAreEyesOpen(false);\n            console.log('No user detected');\n          }\n        } catch (error) {\n          console.error('Error detecting faces:', error);\n        }\n      }\n    };\n\n    const interval = setInterval(async () => {\n      await startDetection();\n    }, 100);\n\n    loadModels();\n    startDetection();\n\n    // Clean up function to stop interval when component unmounts\n    return () => {\n      clearInterval(interval);\n    };\n  }, []);\n\n  const isEyeOpen = (eyeLandmarks) => {\n    const top = eyeLandmarks[1].y;\n    const bottom = eyeLandmarks[5].y;\n    const height = bottom - top;\n    const threshold = height * 0.4; // You can adjust this threshold based on your needs\n    setEyeValue(Math.floor(height));\n\tconsole.log(Math.floor(height),\"Math.floor(height)\")\n    return eyeLandmarks[4].y - eyeLandmarks[1].y > threshold;\n  };\n\n  useEffect(() => {\n    if (eyeValue === 5) {\n      setEyeHeight(true);\n    } else {\n      setEyeHeight(false);\n    }\n  }, [eyeValue]);\n\n  return (\n\t<div style={{height:'765px', background: eyeValue===5 ? 'black' : 'green' }}>\n\t<Webcam\n\t  ref={webcamRef}\n\t  mirrored={true}\n\t  className={`w-[95%] h-full m-auto `}\n\t  style={{ transform: 'scaleX(1)' }}\n\t/>\n\t{/* {eyeHeight ? (\n\t  <p className='text-[#ffffff]'>Eyes are open.</p>\n\t) : (\n\t  <p className='text-[#ffffff]'>Fit yourself in frame.</p>\n\t)} */}\n\t{/* <ProgressBar threshold={eyeValue}/> */}\n  </div>\n  );\n};\n\nexport default EyeDetection;\n\n\n\n"],"mappings":";;AAAA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;;AAEA;;AAGA,OAAOA,KAAK,IAAIC,MAAM,EAAEC,SAAS,EAAEC,QAAQ,QAAQ,OAAO;AAC1D,OAAOC,MAAM,MAAM,cAAc;AACjC,OAAO,KAAKC,OAAO,MAAM,aAAa;AACtC,OAAOC,WAAW,MAAM,eAAe;AAAC,SAAAC,MAAA,IAAAC,OAAA;AAExC,MAAMC,YAAY,GAAGA,CAAC;EAAEC;AAAiB,CAAC,KAAK;EAAAC,EAAA;EAC7C,MAAMC,SAAS,GAAGX,MAAM,CAAC,IAAI,CAAC;EAC9B,MAAM,CAACY,WAAW,EAAEC,cAAc,CAAC,GAAGX,QAAQ,CAAC,KAAK,CAAC;EACrD,MAAM,CAACY,SAAS,EAAEC,YAAY,CAAC,GAAGb,QAAQ,CAAC,CAAC;EAC5C,MAAM,CAACc,QAAQ,EAAEC,WAAW,CAAC,GAAGf,QAAQ,CAAC,CAAC;EAC1C,MAAM,CAACgB,QAAQ,EAAEC,WAAW,CAAC,GAAGjB,QAAQ,CAAC,CAAC;EAE1CD,SAAS,CAAC,MAAM;IACd,MAAMmB,UAAU,GAAG,MAAAA,CAAA,KAAY;MAC7B,MAAMhB,OAAO,CAACiB,IAAI,CAACC,gBAAgB,CAACC,WAAW,CAAC,SAAS,CAAC;MAC1D,MAAMnB,OAAO,CAACiB,IAAI,CAACG,iBAAiB,CAACD,WAAW,CAAC,SAAS,CAAC;MAC3D,MAAMnB,OAAO,CAACiB,IAAI,CAACI,iBAAiB,CAACF,WAAW,CAAC,SAAS,CAAC;IAC7D,CAAC;IAED,MAAMG,cAAc,GAAG,MAAAA,CAAA,KAAY;MACjC,IAAIf,SAAS,CAACgB,OAAO,IAAIhB,SAAS,CAACgB,OAAO,CAACC,KAAK,CAACC,UAAU,KAAK,CAAC,EAAE;QACjE,MAAMC,OAAO,GAAGnB,SAAS,CAACgB,OAAO,CAACC,KAAK;QACvC,MAAMG,WAAW,GAAG;UAAEC,KAAK,EAAEF,OAAO,CAACE,KAAK;UAAEC,MAAM,EAAEH,OAAO,CAACG;QAAO,CAAC;QACpE7B,OAAO,CAAC8B,eAAe,CAACJ,OAAO,EAAEC,WAAW,CAAC;QAE7C,IAAI;UACF,MAAMI,UAAU,GAAG,MAAM/B,OAAO,CAC7BgC,cAAc,CAACN,OAAO,EAAE,IAAI1B,OAAO,CAACiC,uBAAuB,CAAC,CAAC,CAAC,CAC9DC,iBAAiB,CAAC,CAAC,CACnBC,mBAAmB,CAAC,CAAC;UAExB,IAAIJ,UAAU,CAACK,MAAM,GAAG,CAAC,EAAE;YACzB,MAAMC,SAAS,GAAGN,UAAU,CAAC,CAAC,CAAC,CAACM,SAAS;YACzC,MAAMC,OAAO,GAAGD,SAAS,CAACE,UAAU,CAAC,CAAC;YACtC,MAAMC,QAAQ,GAAGH,SAAS,CAACI,WAAW,CAAC,CAAC;YACxC,MAAMC,WAAW,GAAGC,SAAS,CAACL,OAAO,CAAC;YACtC,MAAMM,YAAY,GAAGD,SAAS,CAACH,QAAQ,CAAC;YACxC/B,cAAc,CAACiC,WAAW,IAAIE,YAAY,CAAC;UAC7C,CAAC,MAAM;YACLnC,cAAc,CAAC,KAAK,CAAC;YACrBoC,OAAO,CAACC,GAAG,CAAC,kBAAkB,CAAC;UACjC;QACF,CAAC,CAAC,OAAOC,KAAK,EAAE;UACdF,OAAO,CAACE,KAAK,CAAC,wBAAwB,EAAEA,KAAK,CAAC;QAChD;MACF;IACF,CAAC;IAED,MAAMC,QAAQ,GAAGC,WAAW,CAAC,YAAY;MACvC,MAAM3B,cAAc,CAAC,CAAC;IACxB,CAAC,EAAE,GAAG,CAAC;IAEPN,UAAU,CAAC,CAAC;IACZM,cAAc,CAAC,CAAC;;IAEhB;IACA,OAAO,MAAM;MACX4B,aAAa,CAACF,QAAQ,CAAC;IACzB,CAAC;EACH,CAAC,EAAE,EAAE,CAAC;EAEN,MAAML,SAAS,GAAIQ,YAAY,IAAK;IAClC,MAAMC,GAAG,GAAGD,YAAY,CAAC,CAAC,CAAC,CAACE,CAAC;IAC7B,MAAMC,MAAM,GAAGH,YAAY,CAAC,CAAC,CAAC,CAACE,CAAC;IAChC,MAAMxB,MAAM,GAAGyB,MAAM,GAAGF,GAAG;IAC3B,MAAMG,SAAS,GAAG1B,MAAM,GAAG,GAAG,CAAC,CAAC;IAChChB,WAAW,CAAC2C,IAAI,CAACC,KAAK,CAAC5B,MAAM,CAAC,CAAC;IAClCgB,OAAO,CAACC,GAAG,CAACU,IAAI,CAACC,KAAK,CAAC5B,MAAM,CAAC,EAAC,oBAAoB,CAAC;IACjD,OAAOsB,YAAY,CAAC,CAAC,CAAC,CAACE,CAAC,GAAGF,YAAY,CAAC,CAAC,CAAC,CAACE,CAAC,GAAGE,SAAS;EAC1D,CAAC;EAED1D,SAAS,CAAC,MAAM;IACd,IAAIe,QAAQ,KAAK,CAAC,EAAE;MAClBD,YAAY,CAAC,IAAI,CAAC;IACpB,CAAC,MAAM;MACLA,YAAY,CAAC,KAAK,CAAC;IACrB;EACF,CAAC,EAAE,CAACC,QAAQ,CAAC,CAAC;EAEd,oBACDT,OAAA;IAAKuD,KAAK,EAAE;MAAC7B,MAAM,EAAC,OAAO;MAAE8B,UAAU,EAAE/C,QAAQ,KAAG,CAAC,GAAG,OAAO,GAAG;IAAQ,CAAE;IAAAgD,QAAA,eAC5EzD,OAAA,CAACJ,MAAM;MACL8D,GAAG,EAAEtD,SAAU;MACfuD,QAAQ,EAAE,IAAK;MACfC,SAAS,EAAG,wBAAwB;MACpCL,KAAK,EAAE;QAAEM,SAAS,EAAE;MAAY;IAAE;MAAAC,QAAA,EAAAC,YAAA;MAAAC,UAAA;MAAAC,YAAA;IAAA,OACnC;EAAC;IAAAH,QAAA,EAAAC,YAAA;IAAAC,UAAA;IAAAC,YAAA;EAAA,OAOI,CAAC;AAER,CAAC;AAAC9D,EAAA,CA1FIF,YAAY;AAAAiE,EAAA,GAAZjE,YAAY;AA4FlB,eAAeA,YAAY;AAAC,IAAAiE,EAAA;AAAAC,YAAA,CAAAD,EAAA"},"metadata":{},"sourceType":"module","externalDependencies":[]}